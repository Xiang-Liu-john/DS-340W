{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting Up the Working Environment"
      ],
      "metadata": {
        "id": "h9exCmbUp-BF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E12I0tZ_eHms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc35b329-0144-4830-f59d-16c908c61f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/DS340W\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "%cd '/content/drive/MyDrive/DS340W'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRGTICXkgPUe",
        "outputId": "63b59772-285e-4078-88a0-6b74fa3eeebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Extraction from Audio Files"
      ],
      "metadata": {
        "id": "P-pnCzvBqFo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we have a function designed to extract various musical features from audio files using the librosa library. These features include tempo, chroma frequency, mel-frequency cepstral coefficients (MFCCs), and spectral contrast."
      ],
      "metadata": {
        "id": "QC3im9GvqL1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate multiple features of songs\n",
        "def extract_features(file_path):\n",
        "     src, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "     tempo, _ = librosa.beat.beat_track(y=src, sr=sr)\n",
        "     chroma_stft = librosa.feature.chroma_stft(y=src, sr=sr)\n",
        "     mfcc = librosa.feature.mfcc(y=src, sr=sr)\n",
        "     spectral_contrast = librosa.feature.spectral_contrast(y=src, sr=sr)\n",
        "\n",
        "     # Previously we flattened the feature matrix, now we only calculate statistics\n",
        "     chroma_stft_mean = np.mean(chroma_stft, axis=1)\n",
        "     chroma_stft_std = np.std(chroma_stft, axis=1)\n",
        "     mfcc_mean = np.mean(mfcc, axis=1)\n",
        "     mfcc_std = np.std(mfcc, axis=1)\n",
        "     spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
        "     spectral_contrast_std = np.std(spectral_contrast, axis=1)\n",
        "\n",
        "     #Create a feature vector containing statistics\n",
        "     features_vector = np.hstack([tempo, chroma_stft_mean, chroma_stft_std,\n",
        "                                  mfcc_mean, mfcc_std,\n",
        "                                  spectral_contrast_mean, spectral_contrast_std])\n",
        "     return features_vector"
      ],
      "metadata": {
        "id": "q4b2JfewsCxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function delves into the heart of the audio, quantifying the nuances of music into a structured form that can be analyzed and processed by machine learning algorithms."
      ],
      "metadata": {
        "id": "kLexlOc7qmzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Assembling the Dataset"
      ],
      "metadata": {
        "id": "TinGvhI2qPWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we gather the extracted features from both AI-generated and human-created music files, compiling them into a comprehensive dataset with corresponding labels."
      ],
      "metadata": {
        "id": "MHTaxwisqT2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "\n",
        "#Create an empty list to store features and labels\n",
        "data = []\n",
        "\n",
        "# Traverse the music created by AI\n",
        "for file in os.listdir('/content/drive/MyDrive/DS340W/AI_Music'):\n",
        "     file_path = os.path.join('/content/drive/MyDrive/DS340W/AI_Music', file)\n",
        "     features_vector = extract_features(file_path)\n",
        "     # Create a tuple containing feature vectors and labels\n",
        "     sample = (features_vector, 'AI')\n",
        "     data.append(sample)\n",
        "\n",
        "# Traverse music created by humans\n",
        "for file in os.listdir('/content/drive/MyDrive/DS340W/Human_Music'):\n",
        "     file_path = os.path.join('/content/drive/MyDrive/DS340W/Human_Music', file)\n",
        "     features_vector = extract_features(file_path)\n",
        "     # Create a tuple containing feature vectors and labels\n",
        "     sample = (features_vector, 'Human')\n",
        "     data.append(sample)\n",
        "\n",
        "# We first separate the feature vectors and labels\n",
        "features_list = [sample[0] for sample in data] # Get all feature vectors here\n",
        "labels_list = [sample[1] for sample in data] # Get all labels here\n",
        "\n",
        "# Now convert the feature vector to a DataFrame\n",
        "df_features = pd.DataFrame(features_list)\n",
        "\n",
        "# Add label as a new column of DataFrame\n",
        "df_features['label'] = labels_list\n",
        "\n",
        "df = df_features"
      ],
      "metadata": {
        "id": "tB9ZLB_ZrtO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This portion of code walks through the musical repository, translating each piece into a dataset ready to inform and train the keen mind of an AI."
      ],
      "metadata": {
        "id": "6-j8WHqfqi6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preparing Data for Model Training"
      ],
      "metadata": {
        "id": "HR4k_etoqYi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This snippet is about splitting the dataset into training and testing sets. It ensures that the machine learning models have a set of data to learn from as well as a separate set to validate their predictions."
      ],
      "metadata": {
        "id": "suHqlDEdqZ1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume df is your DataFrame containing features and labels\n",
        "X = df.drop('label', axis=1) #Features\n",
        "y = df['label'] # label\n",
        "\n",
        "# Split the data set, 80% for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the size of the split data set\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Test set size:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6w9FKokuNub",
        "outputId": "3579d5f9-4efa-4918-b764-4d10b214c73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (80, 79)\n",
            "Test set size: (20, 79)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By segmenting the data, we provide a solid foundation for the algorithms to train on and later demonstrate their predictive capabilities."
      ],
      "metadata": {
        "id": "qae2A9pkqe7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Random Forest Classification"
      ],
      "metadata": {
        "id": "xxzFbSCbqsj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It trains a Random Forest classifier on the training data and evaluates its performance on the test data. Random Forest is an ensemble learning method that can be very effective for classification tasks."
      ],
      "metadata": {
        "id": "NEX-AFPqq3Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "#Initialize the random forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "#Train model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5Togrr7u_7w",
        "outputId": "148a17f5-57b9-4dba-f3fe-5a97fd8d8d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       1.00      1.00      1.00        12\n",
            "       Human       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Logistic Regression Analysis"
      ],
      "metadata": {
        "id": "HDdPzOQYq7ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Logistic Regression model is applied to the dataset to predict whether the music was created by AI or a human, and its performance metrics are calculated."
      ],
      "metadata": {
        "id": "BMU7ECJLq9v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "#Initialize logistic regression model\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "#Train model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Use the model to make predictions\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "print(classification_report(y_test, y_pred_log_reg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93hqoXGy5ts9",
        "outputId": "49b748cf-1893-49f7-d621-2a4d153677c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.83      0.83      0.83        12\n",
            "       Human       0.75      0.75      0.75         8\n",
            "\n",
            "    accuracy                           0.80        20\n",
            "   macro avg       0.79      0.79      0.79        20\n",
            "weighted avg       0.80      0.80      0.80        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Support Vector Machine (SVM) Classifier"
      ],
      "metadata": {
        "id": "cRYFahFUrB1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code demonstrates the training of a Support Vector Machine (SVM) with a linear kernel to classify the music pieces. The SVM's performance is then evaluated."
      ],
      "metadata": {
        "id": "sLCHwl4jrD6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(kernel='linear')  # You can choose other kernels such as 'rbf', 'poly', etc.\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(classification_report(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-99eI6FjX-0",
        "outputId": "321f5438-ba66-47dd-f0cf-eb929965b939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.85      0.92      0.88        12\n",
            "       Human       0.86      0.75      0.80         8\n",
            "\n",
            "    accuracy                           0.85        20\n",
            "   macro avg       0.85      0.83      0.84        20\n",
            "weighted avg       0.85      0.85      0.85        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SVM classifier had an accuracy of 0.85, with higher precision and recall for AI-created music compared to human. This shows that it was better at identifying AI-created music correctly, but still performed reasonably well overall."
      ],
      "metadata": {
        "id": "jLCc1M4Vr_y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. K-Nearest Neighbors (K-NN) Classification"
      ],
      "metadata": {
        "id": "lISxJoQBrGfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part uses the K-Nearest Neighbors algorithm to classify the music samples. It determines the label of a sample based on the majority vote of its nearest neighbors."
      ],
      "metadata": {
        "id": "fKSBE4jorIKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=3)  # The number of neighbors can be tuned\n",
        "knn_model.fit(X_train, y_train)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "\n",
        "print(\"K-NN Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
        "print(classification_report(y_test, y_pred_knn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFkGB3kAjZi-",
        "outputId": "2f838616-67f1-4200-a162-4e4be2ded6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-NN Accuracy: 0.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       0.83      0.83      0.83        12\n",
            "       Human       0.75      0.75      0.75         8\n",
            "\n",
            "    accuracy                           0.80        20\n",
            "   macro avg       0.79      0.79      0.79        20\n",
            "weighted avg       0.80      0.80      0.80        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With an accuracy of 0.8, K-NN showed similar precision and recall for both AI and human-created music. While not as high-performing as ensemble methods or the decision tree, it still managed to classify the majority of the instances correctly."
      ],
      "metadata": {
        "id": "kvLelX3CsCF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Decision Tree Classifier"
      ],
      "metadata": {
        "id": "lUnNyoVkrK7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree classifier is being utilized. It creates a model that predicts the class of a sample by learning simple decision rules inferred from the training data."
      ],
      "metadata": {
        "id": "Bm_hghZWrT2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree_model = DecisionTreeClassifier(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "y_pred_tree = tree_model.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
        "print(classification_report(y_test, y_pred_tree))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S3Z8AZbjb-G",
        "outputId": "bb89d6d4-dfc9-4592-f935-5ad544c7aa35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.95\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       1.00      0.92      0.96        12\n",
            "       Human       0.89      1.00      0.94         8\n",
            "\n",
            "    accuracy                           0.95        20\n",
            "   macro avg       0.94      0.96      0.95        20\n",
            "weighted avg       0.96      0.95      0.95        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision tree classifier also performed very well with an accuracy of 0.95. It showed perfect recall for human-created music and very high precision for AI-created music. This model is very close to the ensemble methods, suggesting that the dataset might have distinct, well-defined patterns."
      ],
      "metadata": {
        "id": "FQZCtCGBr3bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "DIRP84ATrVWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gradient Boosting Classifier is a powerful ensemble technique that builds one tree at a time and corrects for the mistakes of previous trees."
      ],
      "metadata": {
        "id": "rJph5FpxrYLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(classification_report(y_test, y_pred_gb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIGIj-LAjiPh",
        "outputId": "01717360-4f9c-463e-bc4d-37da8ff5cb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          AI       1.00      1.00      1.00        12\n",
            "       Human       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building and Evaluating a Deep Learning Model for Binary Classification"
      ],
      "metadata": {
        "id": "kAoKPKj7jSF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code outlines creating and evaluating a TensorFlow model for binary classification, starting from data preprocessing with LabelEncoder to model training and assessing its accuracy on our test dataset."
      ],
      "metadata": {
        "id": "Am0t80n6jVks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert tags from string to binary form\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define model\n",
        "model = Sequential([\n",
        "     Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "     Dropout(0.5), # Dropout layer is used to reduce overfitting\n",
        "     Dense(128, activation='relu'),\n",
        "     Dropout(0.5),\n",
        "     Dense(64, activation='relu'),\n",
        "     Dense(1, activation='sigmoid') # Use the sigmoid activation function because this is a two-classification problem\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "               loss='binary_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "# View model structure\n",
        "model.summary()\n",
        "\n",
        "#Train model\n",
        "history = model.fit(X_train, y_train_encoded,\n",
        "                     epochs=20,\n",
        "                     batch_size=32,\n",
        "                     validation_split=0.2) # Use 20% of the training data as validation data\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E2k9JKtgT8k",
        "outputId": "20cfa14e-d194-42c9-df1f-419b7477167d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 256)               20480     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61697 (241.00 KB)\n",
            "Trainable params: 61697 (241.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "2/2 [==============================] - 2s 408ms/step - loss: 12.2470 - accuracy: 0.5000 - val_loss: 8.1021 - val_accuracy: 0.3750\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 8.9320 - accuracy: 0.4688 - val_loss: 6.4176 - val_accuracy: 0.3750\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 10.0444 - accuracy: 0.3906 - val_loss: 4.0681 - val_accuracy: 0.3750\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 147ms/step - loss: 5.3448 - accuracy: 0.5781 - val_loss: 1.8362 - val_accuracy: 0.3750\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 5.4710 - accuracy: 0.5781 - val_loss: 1.1406 - val_accuracy: 0.4375\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 4.9976 - accuracy: 0.4531 - val_loss: 1.1896 - val_accuracy: 0.4375\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 88ms/step - loss: 3.9495 - accuracy: 0.6562 - val_loss: 2.0049 - val_accuracy: 0.3750\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 143ms/step - loss: 5.1251 - accuracy: 0.5156 - val_loss: 2.7929 - val_accuracy: 0.3750\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 4.9754 - accuracy: 0.4844 - val_loss: 2.7522 - val_accuracy: 0.3750\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 5.8268 - accuracy: 0.4531 - val_loss: 1.6694 - val_accuracy: 0.3750\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 3.3777 - accuracy: 0.5156 - val_loss: 0.7616 - val_accuracy: 0.6250\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 108ms/step - loss: 2.7948 - accuracy: 0.6719 - val_loss: 0.4635 - val_accuracy: 0.8750\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 2.5053 - accuracy: 0.5781 - val_loss: 0.4229 - val_accuracy: 0.8125\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 2.0786 - accuracy: 0.5781 - val_loss: 0.5196 - val_accuracy: 0.7500\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 87ms/step - loss: 2.3154 - accuracy: 0.5625 - val_loss: 0.5529 - val_accuracy: 0.7500\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 152ms/step - loss: 1.9642 - accuracy: 0.6250 - val_loss: 0.5415 - val_accuracy: 0.7500\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 113ms/step - loss: 2.3125 - accuracy: 0.6094 - val_loss: 0.5568 - val_accuracy: 0.7500\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 3.3618 - accuracy: 0.5625 - val_loss: 0.5982 - val_accuracy: 0.7500\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 2.9271 - accuracy: 0.5312 - val_loss: 0.4982 - val_accuracy: 0.7500\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 92ms/step - loss: 1.7579 - accuracy: 0.6406 - val_loss: 0.3969 - val_accuracy: 0.8125\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.7608 - accuracy: 0.7000\n",
            "Test Accuracy: 0.699999988079071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code returns the loss value and accuracy of the model on the test dataset, indicating the model's performance in classifying the binary labels."
      ],
      "metadata": {
        "id": "1EVCghunj5F0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Conslusion"
      ],
      "metadata": {
        "id": "8rOBvsAgrkqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance ranking of the classifiers based on accuracy would be:\n",
        "\n",
        "Gradient Boosting & Random Forest (1.0 accuracy)\n",
        "\n",
        "Decision Tree (0.95 accuracy)\n",
        "\n",
        "SVM (0.85 accuracy)\n",
        "\n",
        "Logistic Regression & K-NN (0.8 accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "SS4GGzAcrnuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's clear from the results that the ensemble methods (Gradient Boosting and Random Forest) are the most accurate for this particular task. However, the decision tree also provides a high level of accuracy and may offer advantages in terms of simplicity and interpretability.\n",
        "\n",
        "The Logistic Regression model, while not as accurate as the ensemble methods or the decision tree, still provides reasonable accuracy and might be preferred when interpretability or computational simplicity is more critical.\n",
        "\n",
        "\n",
        "It is important to note that while accuracy is a useful metric, it is not the only consideration. The choice of model can depend on various factors including interpretability, computational cost, and the specific requirements of the task at hand. Additionally, care must be taken to ensure that the models are not overfitting to the training data and that they will generalize well to new, unseen data. Therefore, further evaluation using cross-validation or on an independent test set would be advisable before deploying any of these models into production."
      ],
      "metadata": {
        "id": "KfJ_wTgAs3BW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning models often require large amounts of data to learn complex features and patterns. If the data set is relatively small, the deep learning model may not be able to fully demonstrate its performance advantages, especially compared with some traditional machine learning algorithms that have less stringent data size requirements."
      ],
      "metadata": {
        "id": "etbVw1PMgWvJ"
      }
    }
  ]
}